--- 
- uri: http://boozeandmescaline.com
  timestamp: Sat Nov 14 01:12:33 -0800 2009
  author: Andy Kish
  content: |-
    Awesome post. There's a fair amount of discussion of clojure online these days, but not enough idiomatic code!
    
    One improvement you could make--you don't explain the `update-in`, and I had to look at the clojure docs. I'm glad I know about it now. :-)
- uri: http://freegeek.in/
  author: Baishampayan Ghose
  timestamp: Sat Nov 14 14:03:59 -0800 2009
  content: Great post. By the way, you could have replaced the (doseq [a agents] (await a)) part with (apply await agents). Much cleaner, IMHO :)
- uri: ""
  timestamp: Sun Nov 15 01:46:16 -0800 2009
  author: Ivan Toshkov
  content: |-
    <p>What is wrong with the straightforward implementation? Something like this:</p>
    
    <pre class="code">(def line-re #&quot;GET /([^ ]*) &quot;)
    
    (defn match-line [line]
      (if-let [[_ hit] (re-find line-re line)]
        {hit 1}))
    
    (defn find-widely-2 [filename]
      (apply merge-with + (pmap match-line (line-seq (reader filename)))))</pre>
    
    <p>I think your regexp is broken.  It matches only pages, whose names are numbers.  The one I'm using above is probably too permissive, but that's not the main point of this, right?</p>
- uri: http://milo.com/
  author: John
  timestamp: Sun Nov 15 23:11:04 -0800 2009
  content: |-
    <p>Why not simply use pmap, like so?</p>
    
    <pre class="code">(ns my-wide-finder
      &quot;A basic map/reduce approach to the wide finder using agents.
      Optimized for being idiomatic and readable rather than speed.
      NOTE: Originally from:
      http://technomancy.us/130
      but updated to use pmap.&quot;
      (:use [clojure.contrib.duck-streams :only [reader]]))
    
    (def re #&quot;GET /(\d+) &quot;)
    
    (defn count-line
      &quot;Increment the relevant entry in the counts map.&quot;
      [line]
      (if-let [[_ hit] (re-find re line)]
        {hit 1}
        {}))
    
    (defn my-find-widely
      &quot;Return a map of pages to hit counts in filename.&quot;
      [filename]
      (apply merge-with +
             (pmap count-line (line-seq (reader filename)))))</pre>
    
    <p>It's less code, easier to understand and I'm guessing more idiomatic.  The only downside is you can't control the number of cores.</p>
    
    <p>It's also substantially faster.  Running them both with all 4 cores on my machine (the pmap version defaults to using all cores I assume) wrapped in a call to &quot;time&quot; I get this:</p>
    
    <p>&quot;Elapsed time: 32508.140029 msecs&quot;</p>
    
    <p>vs the agent approach which yields:</p>
    
    <p>&quot;Elapsed time: 297367.898428 msecs&quot;</p>
    
    <p>Both timings were taken after running each once before to prime the filesystem cache, etc.</p>
    
    <p>Also both approaches are still heavily I/O bound and I haven't done anything to address the performance vs. straightforwardness tradeoffs so there's not really much to be gained over writing it to run on a single core with regular map, which yields:</p>
    
    <p>&quot;Elapsed time: 33095.086899 msecs&quot;</p>
- uri: http://technomancy.us
  timestamp: Mon Nov 16 11:37:15 -0800 2009
  author: Phil
  content: |-
    <p>Baishampayan: Nice; should have read the docs for await more closely.</p>
    
    <p>Ivan: the regex was modified to work with URLs from my blog rather than Tim's.</p>
    
    <p>Ivan and John: Wow; I had no idea how drastic the difference would be between pmap and agents. It's significantly simpler, which is a big win. I'm going to leave the agents version up since I think it's an interesting explanation of how functions get passed around and called with a different set of args depending on the context, but that's definitely interesting.</p>
    
    <p>John: it would be interesting to compare CPU time usage between pmap and regular map rather than just wall-clock time to determine the bottleneck.</p>
- uri: ""
  author: SI Hawakaya
  timestamp: Fri Nov 20 07:06:21 -0800 2009
  content: "Spelling: &quot;na\xC3\xAFve&quot;, not &quot;na\xC3\xAFeve&quot;."
- uri: http://www.tbray.org/ongoing/
  timestamp: Sat Nov 21 09:23:17 -0800 2009
  author: Tim Bray
  content: "Hey, commenter John, it'd be nice to know who you are.  I ran your elegant ultra-minimal code on the Wide Finder machine and it started out pretty well, using on average 5 or 6 of the cores.  But eventually it got stuck in gc hell or some other problem (I wasn't running a lot of diagnostics) so that it had gone single-threaded, so eventually I killed it.  It had burned 32 CPU-hours in about 16 hours elapsed and I don't know how far it got.\r\n\
    \r\n\
    Anybody who wants to dive deeper on this can have an account on the wide-finder machine. "
- uri: http://milo.com/
  author: John
  timestamp: Thu Nov 26 00:36:50 -0800 2009
  content: |-
    Hey Tim,
    
    That'd be me.  Unfortunately I don't get much time to hack on these types of things but if you want to email me at (my first name @ milo dot com) and let me know how to get at the full data set I'll try to dig deeper next time I find myself with some free time.
    
    -
    John
- uri: http://milo.com/
  timestamp: Fri Nov 27 23:57:21 -0800 2009
  author: John
  content: |-
    Oops, provided the wrong version of the chunked code before, I think (can't see the comment yet, but I'm pretty sure...)
    
    Anyway, here's the correct code:
    
    http://pastie.org/pastes/717940
- uri: ""
  author: Ivan Toshkov
  timestamp: Tue Dec 01 05:21:40 -0800 2009
  content: |-
    @Tim Bray: I'm not &quot;John&quot;, but my solution was virtually identical with his, so I'll comment a bit.  The implementation was supposed to be idiomatic, rather than optimized for speed or memory usage or anything else.
    
    One of the problems is with the &quot;apply&quot; function, which will try to compute all the arguments before it calls &quot;merge-with&quot;.  A bit nicer solution would be something like:
    
    (reduce (partial merge-with +) (pmap count-line (line-seq (reader filename))))
    
    It would be interesting to see how this performs, but I don't have the time to play with Wide finder right now.  Still I'm working on a similar problem and if I'm able to optimize it I'll post some results.
- uri: http://milo.com/
  author: John
  timestamp: Thu Nov 26 00:43:24 -0800 2009
  content: |-
    P.S. Just for reference sake, here is the faster version I mentioned to you on #clojure which batches chunks of 20 URLs per unit of work:
    
    http://pastie.org/715782
